{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "#Load the  pre-trained model and tokenizer\n",
        "model_name = \"gpt2-large\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "#Get the input as prompt\n",
        "prompt = input()\n",
        "\n",
        "#Encode the prompt into tokens\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "#Check if pad_token_id is None and set it to eos_token_id if necessary\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "\n",
        "#Generate the text using the input\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,  #Pass the attention mask\n",
        "    max_length=500,  #Length of the generated text\n",
        "    num_return_sequences=1,  #Number of sequences to generate\n",
        "    no_repeat_ngram_size=2,  #Prevent repetition of n-grams\n",
        "    top_k=50,  #Top-k sampling\n",
        "    top_p=0.95,  #Top-p sampling\n",
        "    temperature=1.0,  #Sampling temperature\n",
        "    do_sample=True,  #Random sampling\n",
        "    pad_token_id=tokenizer.eos_token_id  #Set pad_token_id to eos_token_id\n",
        ")\n",
        "\n",
        "#Convert the output into text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfQlJJxE6IiE",
        "outputId": "843b6317-ac0d-4fa7-c964-cd5bd74a4a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is large language model\n",
            "What is large language model that helps in solving our problem?\n",
            "\n",
            "I think it is related to our choice in language, our language choice has to do with the fact that, we don't have to worry about how to deal with large number of entities. What is important is we are allowed to scale up without worrying about performance. We can keep using smaller size of nodes, small nodes are much better because of that.\n",
            " to give more complex programs, let's use bigger language to solve it. I see an interesting challenge in this area is to provide a model for programming languages. When you think about complexity of something, think how many states there are. In C++, it was a lot more than C. It was in Java, or C#. So, there is many problems when we try to address them with more and more data structures, I think, to think of the question from C to C ++ is pretty much equivalent to thinking about language structure of C+ vs C with its smaller language size. Because I am not saying it's not a problem, and of course it really is, but I don a great problem that is a simple question. You can have any big problem you are willing to try and make it smaller, you can always find a tool that makes it easier than what would be considered big problems, if you actually want to focus on real problem.\n"
          ]
        }
      ]
    }
  ]
}